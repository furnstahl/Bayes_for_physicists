{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Statistics for Physicists: A01 Appendices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents of the BSFP series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li><a href=\"BSFP_01_Overview_and_setup.ipynb\">01 Overview and Setup</a>\n",
    "    <li><a href=\"BSFP_02_Basics.ipynb\">02 Bayesian basics</a>\n",
    "    <li><a href=\"BSFP_03_Choosing_priors.ipynb\">03 Choosing priors</a>\n",
    "    <li><a href=\"BSFP_04_Bayesian_updating.ipynb\">04 Examples of Bayesian updating</a>\n",
    "    <li><a href=\"BSFP_05_Sampling.ipynb\">05 Sampling</a>\n",
    "    <li><a href=\"BSFP_06_Model_selection_and_mixing.ipynb\">06 Model selection, mixing, ...</a>\n",
    "    <li><a href=\"BSFP_07_Gaussian_processes.ipynb\">07 Gaussian processes</a>\n",
    "    <li><a href=\"BSFP_08_Machine_learning.ipynb\">08 Machine learning</a>\n",
    "    <li><a href=\"BSFP_A01_Appendices.ipynb\">A01 Appendices: reference, vocabulary, notation</a>\n",
    "</ul>\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"Overview\">Overview of A01 Appendices</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last revised: 14-Oct-2018 by Dick Furnstahl [furnstahl.1@osu.edu].\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"Contents\">Contents of A01 Appendices</a> \n",
    "\n",
    "<ul>\n",
    "    <li><a href=\"#References\">References</a>\n",
    "    <li><a href=\"#Vocabulary\">Vocabulary</a>\n",
    "    <li><a href=\"#Notation\">Notation</a> \n",
    "</ul>\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"Python\">Python/Jupyter set up</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See <a href=\"BSFP_01_Overview_and_setup.ipynb\">Part 01</a> for overall installation and setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# set up for plots in this notebook using matplotlib (there are other plotting choices)\n",
    "%matplotlib inline   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/furnstah/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import norm, uniform\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#plt.style.use('seaborn') # pretty matplotlib plots\n",
    "\n",
    "import corner\n",
    "import pymc3 as pm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make font adjustments\n",
    "#plt.rcParams['font.size'] = 12\n",
    "#plt.rcParams['legend.fontsize'] = 'medium'\n",
    "#plt.rcParams['figure.titlesize'] = 'medium'\n",
    "plt.rcdefaults()  # revert to defaults for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<!-- Use html cell magic to add css styling -->\n",
       "<style>\n",
       "  em {\n",
       "      color: red;\n",
       "  }\n",
       "  dd {\n",
       "      margin-left: 15px;\n",
       "  }\n",
       "  .red{color: red}\n",
       "  .blue{color: blue}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html  \n",
    "<!-- Use html cell magic to add css styling -->\n",
    "<style>\n",
    "  em {\n",
    "      color: red;\n",
    "  }\n",
    "  dd {\n",
    "      margin-left: 15px;\n",
    "  }\n",
    "  .red{color: red}\n",
    "  .blue{color: blue}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"Appendices\">Appendices</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### <a name=\"References\">References</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Please suggest additional references (with links)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "\n",
    "\n",
    "### Physics-oriented pedagogical articles and texts\n",
    "\n",
    "<ul>\n",
    "    <li>R. Trotta,\n",
    "        <a href=\"https://www.tandfonline.com/doi/abs/10.1080/00107510802066753\"><i>Bayes in the sky: Bayesian inference and model selection in cosmology</i></a>, Contemp. Phys. <b>49</b>, 71 (2008)\n",
    "        [<a href=\"https://arxiv.org/abs/0803.4089\">arXiv:0803.4089</a>].\n",
    "        \n",
    "    <li>D.S. Sivia and J. Skilling,\n",
    "       <a href=\"https://www.amazon.com/Data-Analysis-Bayesian-Devinderjit-Sivia/dp/0198568320/ref=mt_paperback?_encoding=UTF8&me=&qid=\"><i>Data Analysis: A Bayesian Tutorial, 2nd edition</i></a>, (Oxford University Press, 2006).\n",
    "    \n",
    "    <li>P. Gregory,\n",
    "     <a href=\"https://www.amazon.com/Bayesian-Logical-Analysis-Physical-Sciences/dp/0521150124/ref=sr_1_1?s=books&ie=UTF8&qid=1538587731&sr=1-1&keywords=gregory+bayesian\"><i>Bayesian Logical Data Analysis for the Physical Sciences: A Comparative Approach with MathematicaÂ® Support</i></a>, (Cambridge University Press, 2010).\n",
    "</ul>    \n",
    "\n",
    "### Standard statistics references\n",
    "\n",
    "<ul>\n",
    "    <li>A. Gelman et al.,\n",
    "        <a href=\"https://www.amazon.com/Bayesian-Analysis-Chapman-Statistical-Science/dp/1439840954/ref=sr_1_1?ie=UTF8&qid=1538589213&sr=8-1&keywords=gelman+bayesian+data+analysis\"><i>Bayesian Data Analysis, 3rd edition</i>, (Chapman and Hall/CRC, 2013).\n",
    "</ul>\n",
    "\n",
    "### BUQEYE references\n",
    "\n",
    "<ul>\n",
    "  <li>R.J. Furnstahl, D.R. Phillips and S. Wesolowski,\n",
    "      <i>A recipe for EFT uncertainty quantification in nuclear physics</i>,\n",
    "      J. Phys. G <b>42</b>, 034028 (2015), [<a href=\"https://arxiv.org/abs/1407.0657\">arXiv:1407.0657</a>].\n",
    "      \n",
    "  <li> R.J. Furnstahl, N. Klco, D.R. Phillips and S.Wesolowski,\n",
    "    <i>Quantifying truncation errors in effective field theory</i>,\n",
    "    Phys. Rev. C <b>92</b>, 024005 (2015)\n",
    "    [<a href=\"https://arxiv.org/abs/1506.01343\">arXiv:1506.01343</a>].\n",
    "\n",
    "  <li>S. Wesolowski, N. Klco, R.J. Furnstahl, D.R. Phillips and A. Thapaliya,\n",
    "  <i>Bayesian parameter estimation for effective field theories</i>,\n",
    "    J. Phys. G <b>43</b>, 074001 (2016)\n",
    "    [<a href=\"https://arxiv.org/abs/1511.03618\">arXiv:1511.03618</a>].\n",
    "    \n",
    "  <li>  J.A. Melendez, S. Wesolowski and R.J. Furnstahl,\n",
    "    <i>Bayesian truncation errors in chiral effective field theory: nucleon-nucleon observables</i>,\n",
    "  Phys. Rev. C <b>96</b>, 024003 (2017)\n",
    "  [<a href=\"https://arxiv.org/abs/1704.03308\">arXiv:1704.03308</a>].\n",
    "    \n",
    "  <li>  S. Wesolowski, R.J. Furnstahl, J.A. Melendez and D.R. Phillips,\n",
    "  <i>Exploring Bayesian parameter estimation for chiral effective field theory using nucleon-nucleon phase shifts</i>,\n",
    "  [<a href=\"https://arxiv.org/abs/1808.08211\">arXiv:1808.08211</a>].\n",
    "\n",
    "    \n",
    "</ul>    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Github repositories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Please suggest more!\n",
    "\n",
    "<ul>\n",
    "  <li>https://github.com/jakevdp/BayesianAstronomy Materials for the Bayesian Methods in Astronomy workshop at the 227th American Astronomical Society meeting.  Includes Jupyter notebooks and useful exercises.  \n",
    "      \n",
    "  <li>http://people.duke.edu/~ccc14/sta-663-2018/ STA 663: Computational Statistics and Statistical Computing (2018) at Duke University.  Lots of good things here!    \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### <a name=\"Vocabulary\">Vocabulary</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Plan: build up a good set of definitions with appropriate links.  Please add more words/phrases!\n",
    "\n",
    "<dl>\n",
    "<dt>conjugate prior </dt>\n",
    "    <dd>If the probability distribution family (e.g., beta distributions) for the posterior pdf is the same as for the prior pdf, the latter is said to be a <a href=\"https://en.wikipedia.org/wiki/Conjugate_prior\">conjugate prior</a>. This means that the updating by Bayes' rule can be carried out analytically.   Some Bayesian practitioners are strongly opposed to the use of conjugate priors (see <a href=\"https://github.com/jakevdp/BayesianAstronomy/blob/master/Index.ipynb\"> comments here</a>). </dd>\n",
    "\n",
    "<!--\n",
    "<dt>contingent </dt>\n",
    "    <dd> </dd>\n",
    "-->\n",
    "\n",
    "<dt>credible vs. confidence interval </dt>\n",
    "    <dd>This is a contrast between Bayesian and frequentist statistics.  For a frequentist, a parameter has a true value, which is fixed and not a distribution.  A 95% confidence interval mean that with a large number of repeated trials, 95% of the calculated confidence intervals would include the true value. This is clearly hard to think about!  A Bayesian 95% credible interval is the range of the posterior for the parameter (which is treated as a random variable) that has 95% of the probability.  So there is a 95% probability that the parameter is in that interval. \n",
    "</dd>\n",
    "\n",
    "<dt>evidence </dt>\n",
    "    <dd>In the standard context of inferring parameters $\\boldsymbol{\\theta}$ given data $D$ and information $I$, the evidence is $p(D\\mid I) = \\int\\! d\\boldsymbol{\\theta}\\, p(D \\mid \\boldsymbol{\\theta},I)\\,p(\\boldsymbol{\\theta},I)$.  This is also called the Fully Marginalized Likelihood or FML. The expression shows that it is the integral over <i>all</i> $\\boldsymbol{\\theta}$ weighted by the likelihood.  This is typically an expensive integral to do.  In the context of model fitting (i.e., parameter estimation), it acts as a normalization constant and in most cases can be ignored because the normalization can be found directly (or only relative probabilities are needed). </dd>\n",
    "\n",
    "<dt>gaussian process </dt>\n",
    "    <dd>\n",
    "From [Wikipedia](https://en.wikipedia.org/wiki/Gaussian_process): \"In probability theory and statistics, a Gaussian process is a stochastic process (a collection of random variables indexed by time or space), such that every finite collection of those random variables has a multivariate normal distribution, i.e. every finite linear combination of them is normally distributed. The distribution of a Gaussian process is the joint distribution of all those (infinitely many) random variables, and as such, it is a distribution over functions with a continuous domain, e.g. time or space.\"\n",
    "</dd>\n",
    "\n",
    "<dt>hierarchical model </dt>\n",
    "    <dd>A model with hyperparameters. See [Wikipedia](https://en.wikipedia.org/wiki/Bayesian_hierarchical_modeling). </dd>\n",
    "\n",
    "<dt>hyperparameter </dt>\n",
    "    <dd>A parameter of a prior distribution. </dd>\n",
    "\n",
    "<dt>iid (independently and identically distributed) </dt>\n",
    "    <dd>A set of random variables is iid (or i.i.d. or IID) if each random variable has the same probability distribution and all are mutually independent. \n",
    "    </dd>\n",
    "\n",
    "<dt>likelihood </dt>\n",
    "    <dd>Usually in the form $p(D\\mid \\boldsymbol{\\theta},I)$, where $\\boldsymbol{\\theta}$ are the parameters of our model, $D$ is the data, and $I$ is any other information we use.  This is the probability of observing our actual data given the model (with the particular parameters $\\boldsymbol{\\theta}$). It is the same quantity that is maximized in frequentist maximum-likelihood approaches. </dd>\n",
    "\n",
    "<dt>MAP estimate</dt>\n",
    "    <dd><a href=\"https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation\">Maximum a posteriori estimate</a>. This is the mode (maximum) of the posterior distribution for the quantity of interest. If the prior is uniform, the MAP estimate equals the maximum likelihood estimate. </dd>\n",
    "\n",
    "<dt>maximum entropy </dt>\n",
    "    <dd>A method used to determine priors. </dd>\n",
    "\n",
    "<dt>MCMC </dt>\n",
    "    <dd>Markov-chain Monte Carlo. A generic name for stochastic sampling methods.  </dd>\n",
    "\n",
    "<dt>model selection and model averaging </dt>\n",
    "    <dd> </dd>\n",
    "\n",
    "<dt>nugget </dt>\n",
    "    <dd>\n",
    "For Gaussian process (GP) calculations or any sampling of a multivariate normal distribution, one typically needs to find the Cholesky decomposition of the covariance matrix.  However, this matrix can become ill-conditioned (too-small or negative eigenvalues).   A standard solution is to add a small number, called a nugget, to the diagonal of the covariance matrix. For GP regression, this is equivalent to adding (or increasing, if already present) the data noise.\n",
    "</dd>\n",
    "\n",
    "<dt>nuisance parameter </dt>\n",
    "    <dd>A nuisance parameter is a parameter in your model whose value you don't care about for the posterior. So you integrate it out (marginalize). </dd>\n",
    "    \n",
    "<dt>overfitting and underfitting</dt>\n",
    "    <dd>This example from http://scikit-learn.org/stable/auto_examples/model_selection/plot_underfitting_overfitting.html of fitting polynomials to nonlinear functions illustrates overfitting and underfitting.  The true function is a cosine with noise added.  A polynomial of degree 1 is an inadequate model for this data; this is underfitting.  The polynomial of degree 15 tries to fit the noise; this is overfitting.  \n",
    "\n",
    "  <img src=\"figures/sphx_glr_plot_underfitting_overfitting_001.png\" width=500>\n",
    "   </dd>\n",
    "\n",
    "<dt>point estimate (cf. interval estimate) </dt>\n",
    "    <dd>A point estimate is a single value to characterize a\n",
    "posterior.  It could be the mode, mean, median or something\n",
    "else.  An interval estimate is more natural in Bayesian statistics, because the full posterior is the real target.  Giving a series of credible intervals often conveys much of the information about the posterior.</dd>\n",
    "\n",
    "<dt>posterior </dt>\n",
    "    <dd>This is the quantity of the left side of Bayes' rule, the thing we want to compute.  Often in the form $p(\\boldsymbol{\\theta}\\mid D,I)$, where $\\boldsymbol{\\theta}$ are the parameters of our model, $D$ is the data, and $I$ is any other information we use. It is our knowledge of the model given the data and any relevant background knowledge (which include the choice of model). </dd>\n",
    "\n",
    "<dt>prior </dt>\n",
    "    <dd>A pdf that encodes what is known about the answer (e.g., parameters) before any data is used. The notation consistent with our definitions of <i>posterior</i> and <i>likelihood</i> is $p(\\boldsymbol{\\theta}\\mid I)$, where $\\boldsymbol{\\theta}$ are the parameters of our model and $I$ is any other information we use (e.g., some of the parameters must be positive or less than a known magnitude because of physics reasons).\n",
    "     See also <i>conjugate prior</i> and <i>maximum entropy</i>.\n",
    "    </dd>\n",
    "\n",
    "<dt>residual</dt>\n",
    "    <dd>The difference of theory prediction and experimental data.\n",
    "    </dd>\n",
    "\n",
    "<dt> </dt>\n",
    "    <dd> </dd>\n",
    "\n",
    "   \n",
    "</dl>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### <a name=\"Notation\">Notation</a>  &nbsp;&nbsp;<span class=\"red\">[still coming . . .]</span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Plan: build up a dictionary of notation with appropriate links and examples (with code)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "univariate normal distribution\n",
    "$$\\mathcal{N}(\\mu,\\sigma^2)$$\n",
    "\n",
    "\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<p>[Return to <a href=\"#Contents\">Contents</a>]</p>\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
